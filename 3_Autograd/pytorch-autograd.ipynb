{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7277ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18471f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Available: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"No GPU using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ece46",
   "metadata": {},
   "source": [
    "sample gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48429e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad -> True - will be calculating derivative\n",
    "# default False\n",
    "x = torch.tensor(3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3fc1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73e1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.sin(y)  # z = sin(y) = sin(x^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13686334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor(3., requires_grad=True)\n",
      "Y:  tensor(9., grad_fn=<PowBackward0>)\n",
      "Z:  tensor(0.4121, grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", x)\n",
    "print(\"Y: \", y)\n",
    "print(\"Z: \", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa34e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.backward()  # dy/dx\n",
    "# gradient calculated in backward direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "902da396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.grad # dy/dx at x=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b77e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() # dz/dx = dz/dy * dy/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154b6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.4668)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad  # dz/dx at x=3.0\n",
    "\n",
    "# does not compute gradients for non-leaf nodes\n",
    "# y.grad  # None, because y is not a leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb60a48",
   "metadata": {},
   "source": [
    "Neural Network - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fcd23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating gradients for neural networks\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f6bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = wx + b\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_pred, y):\n",
    "    epsilon = 1e-7  # to avoid log(0)\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "    return - (y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c44e3750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:  tensor(2., requires_grad=True)\n",
      "b:  tensor(1., requires_grad=True)\n",
      "x:  tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "print(\"W: \", w)\n",
    "print(\"b: \", b)\n",
    "print(\"x: \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f88f93ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0010, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w * x + b  # linear function\n",
    "y_pred = sigmoid(z)  # activation function\n",
    "loss = binary_cross_entropy(y_pred, y)  # loss function\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5efcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation to compute gradients\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21624276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before backpropagation:  7.000970363616943\n",
      "Gradient w.r.t W:  2.9974443912506104\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss before backpropagation: \", loss.item())\n",
    "print(\"Gradient w.r.t W: \", w.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b25fe",
   "metadata": {},
   "source": [
    "clearing grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple time executing gradients will be accumulated\n",
    "# w.grad += w.grad\n",
    "# b.grad += b.grad\n",
    "\n",
    "# to clear gradients\n",
    "w.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5564d880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss without gradient tracking:  7.000970363616943\n"
     ]
    }
   ],
   "source": [
    "# To disable gradient tracking\n",
    "\n",
    "# 1. require_grad_ = False # in-place\n",
    "# w.requires_grad_(False)\n",
    "\n",
    "# 2. detach() - create new tensor without gradient tracking\n",
    "# w_detached = w.detach()\n",
    "\n",
    "# 3. no_grad() - context manager\n",
    "with torch.no_grad():\n",
    "    y_pred = sigmoid(forward(x))\n",
    "    loss = binary_cross_entropy(y_pred, y)\n",
    "    print(\"Loss without gradient tracking: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e3565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
